{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:sd-webui/nataili.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd nataili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class BridgeData(object):\n",
    "    def __init__(self):\n",
    "        random.seed()\n",
    "        self.horde_url = \"https://stablehorde.net\"\n",
    "        # Give a cool name to your instance\n",
    "        self.worker_name = f\"Automated Instance #{random.randint(-100000000, 100000000)}\"\n",
    "        # The api_key identifies a unique user in the horde\n",
    "        self.api_key = \"0000000000\"\n",
    "        # Put other users whose prompts you want to prioritize.\n",
    "        # The owner's username is always included so you don't need to add it here, unless you want it to have lower priority than another user\n",
    "        self.priority_usernames = []\n",
    "        self.max_power = 8\n",
    "        self.nsfw = True\n",
    "        self.censor_nsfw = False\n",
    "        self.blacklist = []\n",
    "        self.censorlist = []\n",
    "        self.allow_img2img = True\n",
    "        self.allow_unsafe_ip = True\n",
    "        self.model_names = [\"stable_diffusion\"]\n",
    "        self.max_pixels = 64*64*8*self.max_power\n",
    "        self.interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, os, time, argparse, urllib3, time,base64,re\n",
    "\n",
    "from nataili.inference.compvis.img2img import img2img\n",
    "from nataili.model_manager import ModelManager\n",
    "from nataili.inference.compvis.txt2img import txt2img\n",
    "from nataili.util.cache import torch_gc\n",
    "from nataili.util import logger,set_logger_verbosity, quiesce_logger, test_logger\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageFilter, ImageOps, ImageChops, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "from base64 import binascii\n",
    "\n",
    "import random\n",
    "model = ''\n",
    "max_content_length = 1024\n",
    "max_length = 80\n",
    "current_softprompt = None\n",
    "softprompts = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@logger.catch(reraise=True)\n",
    "def bridge(interval, model_manager, bd):\n",
    "    horde_url = bd.horde_url # Will replace later\n",
    "    current_id = None\n",
    "    current_payload = None\n",
    "    loop_retry = 0\n",
    "    while True:\n",
    "        if loop_retry > 10 and current_id:\n",
    "            logger.error(f\"Exceeded retry count {loop_retry} for generation id {current_id}. Aborting generation!\")\n",
    "            current_id = None\n",
    "            current_payload = None\n",
    "            current_generation = None\n",
    "            loop_retry = 0\n",
    "        elif current_id:\n",
    "            logger.debug(f\"Retrying ({loop_retry}/10) for generation id {current_id}...\")\n",
    "        available_models = model_manager.get_loaded_models_names()\n",
    "        gen_dict = {\n",
    "            \"name\": bd.worker_name,\n",
    "            \"max_pixels\": bd.max_pixels,\n",
    "            \"priority_usernames\": bd.priority_usernames,\n",
    "            \"nsfw\": bd.nsfw,\n",
    "            \"blacklist\": bd.blacklist,\n",
    "            \"models\": available_models,\n",
    "            \"allow_img2img\": bd.allow_img2img,\n",
    "            \"allow_unsafe_ip\": bd.allow_unsafe_ip,\n",
    "            \"bridge_version\": 3,\n",
    "        }\n",
    "        # logger.debug(gen_dict)\n",
    "        headers = {\"apikey\": bd.api_key}\n",
    "        if current_id:\n",
    "            loop_retry += 1\n",
    "        else:\n",
    "            try:\n",
    "                pop_req = requests.post(horde_url + '/api/v2/generate/pop', json = gen_dict, headers = headers)\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                logger.warning(f\"Server {horde_url} unavailable during pop. Waiting 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            except TypeError:\n",
    "                logger.warning(f\"Server {horde_url} unavailable during pop. Waiting 10 seconds...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            try:\n",
    "                pop = pop_req.json()\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                logger.error(f\"Could not decode response from {horde_url} as json. Please inform its administrator!\")\n",
    "                time.sleep(interval)\n",
    "                continue\n",
    "            if pop == None:\n",
    "                logger.error(f\"Something has gone wrong with {horde_url}. Please inform its administrator!\")\n",
    "                time.sleep(interval)\n",
    "                continue\n",
    "            if not pop_req.ok:\n",
    "                message = pop['message']\n",
    "                logger.warning(f\"During gen pop, server {horde_url} responded with status code {pop_req.status_code}: {pop['message']}. Waiting for 10 seconds...\")\n",
    "                if 'errors' in pop:\n",
    "                    logger.warning(f\"Detailed Request Errors: {pop['errors']}\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            if not pop.get(\"id\"):\n",
    "                skipped_info = pop.get('skipped')\n",
    "                if skipped_info and len(skipped_info):\n",
    "                    skipped_info = f\" Skipped Info: {skipped_info}.\"\n",
    "                else:\n",
    "                    skipped_info = ''\n",
    "                logger.debug(f\"Server {horde_url} has no valid generations to do for us.{skipped_info}\")\n",
    "                time.sleep(interval)\n",
    "                continue\n",
    "            current_id = pop['id']\n",
    "            current_payload = pop['payload']\n",
    "        model = pop.get(\"model\", available_models[0])\n",
    "        # logger.info([current_id,current_payload])\n",
    "        use_nsfw_censor = current_payload.get(\"use_nsfw_censor\", False)\n",
    "        if bd.censor_nsfw and not bd.nsfw:\n",
    "            use_nsfw_censor = True\n",
    "        elif any(word in current_payload['prompt'] for word in bd.censorlist):\n",
    "            use_nsfw_censor = True\n",
    "        use_gfpgan = current_payload.get(\"use_gfpgan\", True)\n",
    "        use_real_esrgan = current_payload.get(\"use_real_esrgan\", False)\n",
    "        source_image = pop.get(\"source_image\")\n",
    "        # These params will always exist in the payload from the horde\n",
    "        gen_payload = {\n",
    "            \"prompt\": current_payload[\"prompt\"],\n",
    "            \"height\": current_payload[\"height\"],\n",
    "            \"width\": current_payload[\"width\"],\n",
    "            \"width\": current_payload[\"width\"],\n",
    "            \"seed\": current_payload[\"seed\"],\n",
    "            \"n_iter\": 1,\n",
    "            \"batch_size\": 1,\n",
    "            \"save_individual_images\": False,\n",
    "            \"save_grid\": False,\n",
    "        }\n",
    "        # These params might not always exist in the horde payload\n",
    "        if 'ddim_steps' in current_payload: gen_payload['ddim_steps'] = current_payload['ddim_steps']\n",
    "        if 'sampler_name' in current_payload: gen_payload['sampler_name'] = current_payload['sampler_name']\n",
    "        if 'cfg_scale' in current_payload: gen_payload['cfg_scale'] = current_payload['cfg_scale']\n",
    "        if 'ddim_eta' in current_payload: gen_payload['ddim_eta'] = current_payload['ddim_eta']\n",
    "        if 'denoising_strength' in current_payload and source_image: \n",
    "            gen_payload['denoising_strength'] = current_payload['denoising_strength']\n",
    "        # logger.debug(gen_payload)\n",
    "        req_type = \"txt2img\"\n",
    "        if source_image:\n",
    "            req_type = \"img2img\"\n",
    "        logger.debug(f\"{req_type} ({model}) request with id {current_id} picked up. Initiating work...\")\n",
    "        try:\n",
    "            if source_image:\n",
    "                base64_bytes = source_image.encode('utf-8')\n",
    "                img_bytes = base64.b64decode(base64_bytes)\n",
    "                gen_payload['init_img'] = Image.open(BytesIO(img_bytes))\n",
    "                generator = img2img(model_manager.loaded_models[model][\"model\"], model_manager.loaded_models[model][\"device\"], 'bridge_generations', load_concepts=True, concepts_dir='models/custom/sd-concepts-library')\n",
    "            else:\n",
    "                generator = txt2img(model_manager.loaded_models[model][\"model\"], model_manager.loaded_models[model][\"device\"], 'bridge_generations', load_concepts=True, concepts_dir='models/custom/sd-concepts-library')\n",
    "        except KeyError:\n",
    "            continue\n",
    "        # If the received image is unreadable, we continue\n",
    "        except UnidentifiedImageError:\n",
    "            logger.error(f\"Source image received for img2img is unreadable. Falling back to text2img!\")\n",
    "            if 'denoising_strength' in gen_payload:\n",
    "                del gen_payload['denoising_strength']\n",
    "            generator = txt2img(model_manager.loaded_models[model][\"model\"], model_manager.loaded_models[model][\"device\"], 'bridge_generations', load_concepts=True, concepts_dir='models/custom/sd-concepts-library')\n",
    "        except binascii.Error:\n",
    "            logger.error(f\"Source image received for img2img is cannot be base64 decoded (binascii.Error). Falling back to text2img!\")\n",
    "            if 'denoising_strength' in gen_payload:\n",
    "                del gen_payload['denoising_strength']\n",
    "            generator = txt2img(model_manager.loaded_models[model][\"model\"], model_manager.loaded_models[model][\"device\"], 'bridge_generations', load_concepts=True, concepts_dir='models/custom/sd-concepts-library')\n",
    "        generator.generate(**gen_payload)\n",
    "        torch_gc()\n",
    "      \n",
    "\n",
    "\n",
    "        # images, seed, info, stats = txt2img(**current_payload)\n",
    "        buffer = BytesIO()\n",
    "        # We send as WebP to avoid using all the horde bandwidth\n",
    "        image = generator.images[0][\"image\"]\n",
    "        seed = generator.images[0][\"seed\"]\n",
    "        image.save(buffer, format=\"WebP\", quality=90)\n",
    "        # logger.info(info)\n",
    "        submit_dict = {\n",
    "            \"id\": current_id,\n",
    "            \"generation\": base64.b64encode(buffer.getvalue()).decode(\"utf8\"),\n",
    "            \"api_key\": bd.api_key,\n",
    "            \"seed\": seed,\n",
    "            \"max_pixels\": bd.max_pixels,\n",
    "        }\n",
    "        current_generation = seed\n",
    "        while current_id and current_generation != None:\n",
    "            try:\n",
    "                submit_req = requests.post(horde_url + '/api/v2/generate/submit', json = submit_dict, headers = headers)\n",
    "                try:\n",
    "                    submit = submit_req.json()\n",
    "                except json.decoder.JSONDecodeError:\n",
    "                    logger.error(f\"Something has gone wrong with {horde_url} during submit. Please inform its administrator!  (Retry {loop_retry}/10)\")\n",
    "                    time.sleep(interval)\n",
    "                    continue\n",
    "                if submit_req.status_code == 404:\n",
    "                    logger.warning(f\"The generation we were working on got stale. Aborting!\")\n",
    "                elif not submit_req.ok:\n",
    "                    logger.warning(f\"During gen submit, server {horde_url} responded with status code {submit_req.status_code}: {submit['message']}. Waiting for 10 seconds...  (Retry {loop_retry}/10)\")\n",
    "                    if 'errors' in submit:\n",
    "                        logger.warning(f\"Detailed Request Errors: {submit['errors']}\")\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.info(f'Submitted generation with id {current_id} and contributed for {submit_req.json()[\"reward\"]}')\n",
    "                current_id = None\n",
    "                current_payload = None\n",
    "                current_generation = None\n",
    "                loop_retry = 0\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                logger.warning(f\"Server {horde_url} unavailable during submit. Waiting 10 seconds...  (Retry {loop_retry}/10)\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "        time.sleep(interval)\n",
    "\n",
    "@logger.catch(reraise=True)\n",
    "def check_models(models):\n",
    "    logger.init(\"Models\", status=\"Checking\")\n",
    "    from os.path import exists\n",
    "    import sys\n",
    "    mm = ModelManager()\n",
    "    models_exist = True\n",
    "    not_found_models = []\n",
    "    for model in models:\n",
    "        if not mm.get_model(model):\n",
    "            logger.err(f\"Model name requested {model} in bridgeData is unknown to us. Please check your configuration. Aborting!\")\n",
    "            sys.exit(1)\n",
    "        if not mm.validate_model(model):\n",
    "            models_exist = False\n",
    "            not_found_models.append(model)\n",
    "    if not models_exist:\n",
    "        choice = input(f\"You do not appear to have downloaded the models needed yet.\\nYou need at least a main model to proceed. Would you like to download your prespecified models?\\n\\\n",
    "        y: Download {not_found_models} (default).\\n\\\n",
    "        n: Abort and exit\\n\\\n",
    "        all: Download all models (This can take a significant amount of time and bandwidth)?\\n\\\n",
    "        Please select an option: \")\n",
    "        if choice not in ['y', 'Y', '', 'yes', 'all', 'a']:\n",
    "            sys.exit(1)\n",
    "        needs_hf = False\n",
    "        for model in not_found_models:\n",
    "            dl = mm.get_model_download(model)\n",
    "            for m in dl:\n",
    "                if 'huggingface.co' in m['file_url']:\n",
    "                    needs_hf = True\n",
    "        if needs_hf or choice in ['all', 'a']:\n",
    "            try:\n",
    "                from creds import hf_username,hf_password\n",
    "            except:\n",
    "                hf_username = input(\"Please type your huggingface.co username: \")\n",
    "                hf_password = input(\"Please type your huggingface.co Access Token or password: \")\n",
    "            hf_auth = {\"username\": hf_username, \"password\": hf_password}\n",
    "            mm.set_authentication(hf_auth=hf_auth)\n",
    "        mm.init()\n",
    "        mm.taint_models(not_found_models)\n",
    "        if choice in ['all', 'a']:\n",
    "            mm.download_all()    \n",
    "        elif choice in ['y', 'Y', '', 'yes']:\n",
    "            for model in not_found_models:\n",
    "                logger.init(f\"Model: {model}\", status=\"Downloading\")\n",
    "                if not mm.download_model(model):\n",
    "                    logger.message(\"Something went wrong when downloading the model and it does not fit the expected checksum. Please check that your HuggingFace authentication is correct and that you've accepted the model license from the browser.\")\n",
    "                    sys.exit(0)\n",
    "    logger.init_ok(\"Models\", status=\"OK\")\n",
    "    if exists('./bridgeData.py'):\n",
    "        logger.init_ok(\"Bridge Config\", status=\"OK\")\n",
    "    elif input(\"You do not appear to have a bridgeData.py. Would you like to create it from the template now? (y/n)\") in ['y', 'Y', '', 'yes']:\n",
    "        with open('bridgeData_template.py','r') as firstfile, open('bridgeData.py','a') as secondfile:\n",
    "            for line in firstfile:\n",
    "                secondfile.write(line)\n",
    "        logger.message(\"bridgeData.py created. Bridge will exit. Please edit bridgeData.py with your setup and restart the bridge\")\n",
    "        sys.exit(0)\n",
    "    \n",
    "def load_bridge_data():\n",
    "    bridge_data = BridgeData()\n",
    "    if bridge_data.max_power < 2:\n",
    "        bridge_data.max_power = 2\n",
    "    bridge_data.max_pixels = 64*64*8*bridge_data.max_power\n",
    "    return(bridge_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    bd = load_bridge_data()\n",
    "    # test_logger()\n",
    "    check_models(bd.model_names)\n",
    "    model_manager = ModelManager()\n",
    "    model_manager.init()\n",
    "    for model in bd.model_names:\n",
    "        logger.init(f'{model}', status=\"Loading\")\n",
    "        success = model_manager.load_model(model)\n",
    "        if success:\n",
    "            logger.init_ok(f'{model}', status=\"Loaded\")\n",
    "        else:\n",
    "            logger.init_err(f'{model}', status=\"Error\")\n",
    "    logger.init(f\"API Key '{bd.api_key}'. Server Name '{bd.worker_name}'. Horde URL '{bd.horde_url}'. Max Pixels {bd.max_pixels}\", status=\"Joining Horde\")\n",
    "    try:\n",
    "        bridge(bd.interval, model_manager, bd)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(f\"Keyboard Interrupt Received. Ending Process\")\n",
    "    logger.init(f\"{bd.worker_name} Instance\", status=\"Stopped\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
